{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a50ecf",
   "metadata": {},
   "source": [
    "# Implementación N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81da4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40fe7f",
   "metadata": {},
   "source": [
    "Se definen los PATHS a los datos. En caso de necesitarlo aquí es donde se modifican para poder utilizar los datos locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77643994",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_BAC = \"data/BAC/blogs\"  # Cambia esta ruta por la de tu carpeta con archivos XML\n",
    "OUTPUT_BAC_JSONL = \"data/processed/BAC.jsonl\"  # Ruta del archivo JSONL de salida\n",
    "PATH_TO_20NG = \"data/20news-18828/\"  # Cambia esta ruta por la de tu carpeta con archivos de 20 Newsgroups\n",
    "OUTPUT_20NG_JSONL = \"data/processed/20news.jsonl\"\n",
    "\n",
    "OUTPUT_20NG_TOKENIZED_JSONL = \"data/processed/20news_tokenized.jsonl\"\n",
    "OUTPUT_BAC_TOKENIZED_JSONL = \"data/processed/BAC_tokenized.jsonl\"\n",
    "\n",
    "\n",
    "GROUP_ID = \"0100\"\n",
    "OUTPUT_20NG_SPLITS_JSONL = f\"splits/20N_{GROUP_ID}_training.jsonl\"\n",
    "OUTPUT_BAC_SPLITS_JSONL = f\"splits/BAC_{GROUP_ID}_training.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e190918",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f006952",
   "metadata": {},
   "source": [
    "Para ambos archivos se decidió que la mejor manera de guardar la información era en formato JSONL, de manera que cada línea del archivo es un JSON independiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debb5bb",
   "metadata": {},
   "source": [
    "### Dataset BAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0658b9",
   "metadata": {},
   "source": [
    "Para realizar la unificación de los datos es necesario retirar todas aquellas entidades que generan problemas y que no deberían existir en un archivo MXL. Adicionalmente se retiran todos los caracteres de control que no sean:\n",
    "\n",
    "- Tab (`\\t`)\n",
    "- Newline (`\\n`)\n",
    "- Carriage return (`\\r`)\n",
    "- Caracteres Unicode válidos (acentos,emojis, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c049d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regex: reemplaza & que NO inician entidad válida (&word;, &#123;, &#x1F;)\n",
    "_AMP_FIX = re.compile(r'&(?!#\\d+;|#x[0-9a-fA-F]+;|\\w+;)')\n",
    "\n",
    "# XML 1.0 chars válidos (excluimos controles)\n",
    "# Permitimos: \\x09, \\x0A, \\x0D, \\x20-\\uD7FF, \\uE000-\\uFFFD, \\U00010000-\\U0010FFFF\n",
    "_INVALID_XML_CHARS = re.compile(\n",
    "    r'([^\\x09\\x0A\\x0D\\x20-\\uD7FF\\uE000-\\uFFFD'\n",
    "    r'\\U00010000-\\U0010FFFF])',\n",
    "    flags=re.UNICODE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ea2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_xml_text(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia texto para que sea más amigable al parser XML aplicando múltiples transformaciones.\n",
    "    \n",
    "    Realiza una limpieza en tres pasos:\n",
    "    1. Decodifica entidades HTML comunes (&nbsp;, &hearts;, etc.) a caracteres Unicode\n",
    "    2. Escapa ampersands sueltos (& -> &amp;) que no sean entidades válidas\n",
    "    3. Remueve caracteres inválidos para XML 1.0 según especificación\n",
    "    \n",
    "    Args:\n",
    "        raw (str): Texto crudo que puede contener entidades HTML, ampersands sueltos \n",
    "                   y caracteres inválidos para XML\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto limpio y válido para parsing XML, con entidades decodificadas,\n",
    "             ampersands escapados correctamente y caracteres inválidos removidos\n",
    "    \"\"\"\n",
    "\n",
    "    unescaped = html.unescape(raw)\n",
    "\n",
    "    amp_fixed = _AMP_FIX.sub(\"&amp;\", unescaped)\n",
    "\n",
    "    cleaned = _INVALID_XML_CHARS.sub(\"\", amp_fixed)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def extract_texts(root: etree._Element) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Extrae listas de fechas y posts de un elemento XML, tolerando namespaces.\n",
    "    \n",
    "    Utiliza XPath con local-name() para buscar elementos independientemente \n",
    "    de sus namespaces.\n",
    "    \n",
    "    Args:\n",
    "        root (etree._Element): Elemento raíz del árbol XML desde donde extraer\n",
    "                               los elementos 'date' y 'post'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Tupla conteniendo:\n",
    "            - Lista de strings con fechas encontradas (elementos con local-name='date')\n",
    "            - Lista de strings con posts encontrados (elementos con local-name='post')\n",
    "            Ambas listas tienen texto limpio (strip aplicado) y valores vacíos \n",
    "            para elementos sin contenido textual\n",
    "    \n",
    "    \"\"\"\n",
    "    # Selecciona cualquier elemento cuyo local-name sea 'date' o 'post'\n",
    "    dates = [ (el.text or \"\").strip()\n",
    "              for el in root.xpath(\".//*[local-name()='date']\") ]\n",
    "    posts = [ (el.text or \"\").strip()\n",
    "              for el in root.xpath(\".//*[local-name()='post']\") ]\n",
    "    return dates, posts\n",
    "\n",
    "def xml_to_jsonl(input_dir: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Convierte todos los archivos XML 'sucios' de un directorio a un único archivo JSONL.\n",
    "    \n",
    "    Procesa archivos XML potencialmente malformados o con caracteres inválidos,\n",
    "    aplicando limpieza automática y parsing tolerante. Cada línea del JSONL resultante\n",
    "    corresponde a un post extraído, con su fecha asociada si existe.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Ruta del directorio que contiene archivos XML (*.xml)\n",
    "                         a procesar. Se procesan todos los archivos con extensión .xml\n",
    "        output_path (str): Ruta del archivo JSONL de salida donde se escribirán\n",
    "                          los posts extraídos. Se sobrescribe si ya existe\n",
    "    \n",
    "    Returns:\n",
    "        None: La función no retorna valor, pero imprime estadísticas del procesamiento\n",
    "              y genera el archivo JSONL especificado\n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = str(Path(input_dir) / \"*.xml\")\n",
    "    parser = etree.XMLParser(recover=True, resolve_entities=False)\n",
    "\n",
    "    total_files = 0\n",
    "    parsed_ok = 0\n",
    "    skipped = 0\n",
    "    total_posts = 0\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for path in glob.glob(pattern):\n",
    "            total_files += 1\n",
    "            file_id = Path(path).stem\n",
    "\n",
    "            try:\n",
    "                # leer como texto (ignorar errores de codificación raros)\n",
    "                raw = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                cleaned = clean_xml_text(raw)\n",
    "\n",
    "                # parsear con lxml tolerante\n",
    "                root = etree.fromstring(cleaned.encode(\"utf-8\"), parser=parser)\n",
    "                if root is None:\n",
    "                    # lxml no pudo recuperar nada útil\n",
    "                    skipped += 1\n",
    "                    print(f\"Saltando {path} (irrecuperable tras limpieza)\")\n",
    "                    continue\n",
    "\n",
    "                dates, posts = extract_texts(root)\n",
    "                max_len = max(len(dates), len(posts)) if (dates or posts) else 0\n",
    "\n",
    "                for i in range(max_len):\n",
    "                    rec = {\n",
    "                        \"source_file\": file_id,\n",
    "                        \"index\": i,\n",
    "                        \"date\": dates[i] if i < len(dates) else \"\",\n",
    "                        \"post\": posts[i] if i < len(posts) else \"\",\n",
    "                    }\n",
    "                    out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                    total_posts += 1\n",
    "\n",
    "                parsed_ok += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                # Si algo explota, seguimos con el siguiente pero reportamos\n",
    "                skipped += 1\n",
    "                print(f\"Saltando {path} (falló incluso con recover): {e}\")\n",
    "\n",
    "    print(f\"Archivos totales: {total_files}\")\n",
    "    print(f\"Parseados OK:     {parsed_ok}\")\n",
    "    print(f\"Saltados:         {skipped}\")\n",
    "    print(f\"Posts escritos:   {total_posts}\")\n",
    "    print(f\"Salida JSONL:     {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dd760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xml_to_jsonl(PATH_TO_BAC, OUTPUT_BAC_JSONL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43357d22",
   "metadata": {},
   "source": [
    "### Dataset 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bbac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_files(input_dir: str,recursive: bool=False):\n",
    "    \"\"\"\n",
    "    Itera sobre archivos en un directorio que coincidan con un patrón específico.\n",
    "    \n",
    "    Busca archivos en el directorio especificado usando globbing patterns.\n",
    "    Opcionalmente puede buscar de forma recursiva en subdirectorios.\n",
    "    Los resultados se devuelven ordenados alfabéticamente.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Ruta del directorio donde buscar archivos.\n",
    "        recursive (bool, optional): Si True, busca recursivamente en subdirectorios.\n",
    "                                   Si False, solo busca en el directorio raíz.\n",
    "                                   Defaults to False.\n",
    "    \n",
    "    Yields:\n",
    "        Path: Objeto Path de cada archivo encontrado que coincida con el patrón.\n",
    "    \n",
    "    \"\"\"\n",
    "    base = Path(input_dir)\n",
    "    it = base.rglob(\"*\") if recursive else base.glob(\"*\")\n",
    "    for p in sorted(it):\n",
    "        if p.is_file():\n",
    "            yield p\n",
    "\n",
    "def parse_message(path: Path):\n",
    "    \"\"\"\n",
    "    Parsea un archivo de mensaje extrayendo headers y contenido del cuerpo.\n",
    "    \n",
    "    Lee un archivo de mensaje (formato email/newsgroup) y separa las cabeceras\n",
    "    del cuerpo del mensaje. Maneja cabeceras multi-línea que continúan en\n",
    "    líneas siguientes con espacios o tabs. Intenta diferentes codificaciones\n",
    "    para manejar caracteres especiales.\n",
    "    \n",
    "    Args:\n",
    "        path (Path): Ruta del archivo de mensaje a parsear.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las siguientes claves:\n",
    "            - file_name (str): Nombre del archivo procesado\n",
    "            - subject (str): Asunto del mensaje (header \"Subject\")\n",
    "            - from (str): Remitente del mensaje (header \"From\") \n",
    "            - text (str): Contenido completo del cuerpo del mensaje\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        raw = path.read_text(encoding=\"latin-1\", errors=\"replace\")\n",
    "\n",
    "    lines = raw.splitlines()\n",
    "    headers = {}\n",
    "    body_lines = []\n",
    "    in_headers = True\n",
    "    current_field = None\n",
    "\n",
    "    for line in lines:\n",
    "        if in_headers:\n",
    "            if line.strip() == \"\":\n",
    "                in_headers = False\n",
    "                continue\n",
    "            if line[:1] in (\" \", \"\\t\") and current_field:\n",
    "                # continuación de la cabecera previa\n",
    "                headers[current_field] = (headers[current_field] + \" \" + line.strip()).strip()\n",
    "                continue\n",
    "            if \":\" in line:\n",
    "                name, value = line.split(\":\", 1)\n",
    "                current_field = name.strip().lower()\n",
    "                headers[current_field] = value.strip()\n",
    "            else:\n",
    "                # línea sin \":\" dentro de la zona de cabeceras → asumimos que empezó el cuerpo\n",
    "                in_headers = False\n",
    "                body_lines.append(line)\n",
    "        else:\n",
    "            body_lines.append(line)\n",
    "\n",
    "    return {\n",
    "        \"file_name\": path.name,\n",
    "        \"subject\": headers.get(\"subject\", \"\"),\n",
    "        \"from\": headers.get(\"from\", \"\"),\n",
    "        \"text\": \"\\n\".join(body_lines).strip()\n",
    "    }\n",
    "\n",
    "def folder_to_jsonl(input_dir: str, output_file: str, recursive: bool=False):\n",
    "    \"\"\"\n",
    "    Convierte todos los archivos de mensaje en un directorio a formato JSONL.\n",
    "    \n",
    "    Procesa todos los archivos en el directorio especificado, extrae los datos\n",
    "    de cada mensaje usando parse_message(), y guarda cada mensaje como una línea\n",
    "    JSON en un archivo JSONL. Crea el directorio de salida si no existe.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Ruta del directorio que contiene los archivos de mensaje.\n",
    "        output_file (str): Ruta del archivo JSONL de salida donde guardar los datos.\n",
    "        recursive (bool, optional): Si True, procesa subdirectorios recursivamente.\n",
    "                                   Si False, solo procesa el directorio raíz.\n",
    "                                   Defaults to False.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    out = Path(output_file)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for p in iter_files(input_dir, recursive):\n",
    "            rec = parse_message(p)\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff275da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_to_jsonl(PATH_TO_20NG, OUTPUT_20NG_JSONL, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bd38d",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de2dab",
   "metadata": {},
   "source": [
    "Para la tokenización se utilizó la librería `nltk` tanto para la tokenización de oraciones como de palabras. Se seleccionó esta alternativa por encima de usar expresiones regulares para intentar cubrir la mayor cantidad de casos posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43bb1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087f8e3",
   "metadata": {},
   "source": [
    "Se define una expresión regular para identificar tokens que contengan números y reemplazarlos por el token especial `NUM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029c0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"¿Es número?\" → cualquier token que contenga al menos un dígito\n",
    "_HAS_DIGIT = re.compile(r\".*\\d.*\", flags=re.ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b009a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences_nltk(text: str, language: str = 'english'):\n",
    "    \"\"\"\n",
    "    Tokeniza texto en oraciones usando NLTK.\n",
    "    \n",
    "    Utiliza el tokenizador Punkt de NLTK que está entrenado específicamente\n",
    "    para detectar límites de oraciones en diferentes idiomas.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a tokenizar en oraciones.\n",
    "        language (str, optional): Idioma para el tokenizador. \n",
    "                                Defaults to 'english'.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: Lista de oraciones encontradas en el texto.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    return nltk.sent_tokenize(text, language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9bdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando caracteres no deseados y convirtiendo a minúsculas.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a normalizar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto normalizado.\n",
    "    \"\"\"\n",
    "    t = t.strip()\n",
    "    return re.sub(r\"\\s+\", \" \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text: str, language: str = 'english'):\n",
    "    \"\"\"\n",
    "    Recibe una cadaena de palabras y devuelve los tokens ntlk\n",
    "    de cada una de ellas\n",
    "\n",
    "    Args:\n",
    "        sent (str): La frase a tokenizar.\n",
    "        language (str): el idioma de la frase, \n",
    "                        para el parametro de ntlk\n",
    "\n",
    "    Returns:\n",
    "        (list): la lista de tokens/palabras ntlk\n",
    "    \"\"\"\n",
    "    return tokenize_sentences_nltk(text, language) if text else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede29a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts_from_jsonl(in_path: str, text_key: str = \"text\"):\n",
    "    \"\"\"\n",
    "    Recibe una path y devuleve una lista con todos los valores\n",
    "    de la llave \"text_key\" de cada linea\n",
    "\n",
    "    Args:\n",
    "        in_path (str): Ubicacion del JSONL a procesar.\n",
    "        text_key (str): llave a revisar en cada linea del JSONL \n",
    "\n",
    "    Returns:\n",
    "        texts (list): la lista de los contenidos de text_key de\n",
    "        todos los registros del JSONL\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            texts.append(obj.get(text_key, \"\"))\n",
    "    return texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent: str, language: str = 'english'):\n",
    "    \"\"\"\n",
    "    Recibe una cadaena de palabras y devuelve los tokens ntlk\n",
    "    de cada una de ellas\n",
    "\n",
    "    Args:\n",
    "        sent (str): La frase a tokenizar.\n",
    "        language (str): el idioma de la frase, \n",
    "                        para el parametro de ntlk\n",
    "\n",
    "    Returns:\n",
    "        (list): la lista de tokens/palabras ntlk\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sent, language=language)\n",
    "\n",
    "def map_numbers(tok: str) -> str:\n",
    "    \"\"\"\n",
    "    Recibe una cadaena de caracteres y verifica si tiene numeros para\n",
    "    poder cambiar ese elemento por la cadena de caracteres NUM.\n",
    "\n",
    "    Args:\n",
    "        tok (str): El texto a verificar si tiene numeros.\n",
    "\n",
    "    Returns:\n",
    "        str: NUM si tiene numeros o la palabra.\n",
    "    \"\"\"\n",
    "    return \"NUM\" if _HAS_DIGIT.match(tok) else tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d8ac4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(in_path: str, text_key: str = \"text\") -> Counter:\n",
    "    counts = Counter()\n",
    "    texts = load_texts_from_jsonl(in_path,text_key)\n",
    "    for text in texts:\n",
    "        txt = normalize_text(text.lower())\n",
    "        for sent in split_sentences(txt):\n",
    "            toks = [map_numbers(t) for t in tokenize(sent)]\n",
    "            counts.update(toks)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentences_with_unk(in_path: str, out_path: str, counts: Counter, text_key: str = \"text\"):\n",
    "    \"\"\"\n",
    "    Separa cada uno de los registros de los datasets en oraciones(sentences).\n",
    "    Agrega los caracteres de inicio y de terminacion para cada una de las frases.\n",
    "    Recibe un elemento Counter con el # de apariciones de cada palabra.\n",
    "    Para cada una de las frases, recorre las palabras y cambia por <UNK>\n",
    "    las que salgan 1 sola vez en el Counter\n",
    "\n",
    "    Args:\n",
    "        in_path (str): El path del JSONL a procesar.\n",
    "        out_path (str): El path del JSONL a crear.\n",
    "        counts (Counter): El contador de las palabras.\n",
    "        text_key(str): la llave dentro de cada registro del JSONL que contiene\n",
    "                        el texto en lenguaje natural de interes\n",
    "\n",
    "    Returns:\n",
    "        No retorna nada, pero escribe el archivo JSONL en el path especificado \n",
    "        con las frases ya procesadas.\n",
    "    \"\"\"\n",
    "    texts = load_texts_from_jsonl(in_path, text_key)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for text in texts:\n",
    "            txt = normalize_text(text.lower())\n",
    "            for sent in split_sentences(txt):\n",
    "                toks = [map_numbers(t) for t in tokenize(sent)]\n",
    "                toks = [t if counts[t] > 1 else \"<UNK>\" for t in toks]\n",
    "                if not toks:\n",
    "                    continue\n",
    "                sent_tokens = [\"<s>\", *toks, \"</s>\"]\n",
    "                out.write(json.dumps({\"sentence\": sent_tokens}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb549ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(input_jsonl: str, output_jsonl: str, text_key: str = \"text\"):\n",
    "    counts = count_tokens(input_jsonl, text_key)\n",
    "    write_sentences_with_unk(input_jsonl, output_jsonl, counts, text_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b9f357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_dataset(OUTPUT_20NG_JSONL, \"data/processed/20news_tokenized.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d89bf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_dataset(OUTPUT_BAC_JSONL, \"data/processed/BAC_tokenized.jsonl\",text_key=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050992cb",
   "metadata": {},
   "source": [
    "## División de datos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4fcb0",
   "metadata": {},
   "source": [
    "Con la ayuda de random se realiza una división 80/20 de los datos para entrenamiento y prueba respectivamente. Se asegura que la división sea reproducible utilizando una semilla fija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a09650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(input_jsonl, prefix, group_code, out_dir=\".\", train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Mezcla las frases del archivo JSONL usando la libreria random\n",
    "    Utiliza la semilla para garantizar qeu se pueda reproducir\n",
    "    Se crean los archivos JSONL con los datasets de entrenamiento y\n",
    "    testeo de acuerdo con las proporciones recomendadas y el formato\n",
    "    de nombramiento\n",
    "\n",
    "    Args:\n",
    "        input_jsonl (str): El path del JSONL a procesar.\n",
    "        prefix (str): Para poder reusar la función para los 2 datasets,\n",
    "                        se especifica el nombre para el formateo.\n",
    "        out_dir (str): El path de los JSONL a crear\n",
    "        train_ratio (float): proporcion de datos para entrenamiento\n",
    "        seed (int): semilla para el randomizador \n",
    "\n",
    "    Returns:\n",
    "        No retorna nada, pero escribe los JSONL en el path especificado \n",
    "        con las frases ya procesadas y randomizadas.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # cargar todas las oraciones\n",
    "    sentences = []\n",
    "    with open(input_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            sentences.append(obj)\n",
    "\n",
    "    # barajar\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # dividir 80/20\n",
    "    split_idx = int(len(sentences) * train_ratio)\n",
    "    train_sents = sentences[:split_idx]\n",
    "    test_sents = sentences[split_idx:]\n",
    "\n",
    "    # nombres de salida\n",
    "    out_train = Path(out_dir) / f\"{prefix}_{group_code}_training.jsonl\"\n",
    "    out_test = Path(out_dir) / f\"{prefix}_{group_code}_testing.jsonl\"\n",
    "\n",
    "    # guardar\n",
    "    with open(out_train, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in train_sents:\n",
    "            f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(out_test, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in test_sents:\n",
    "            f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Training: {len(train_sents)} → {out_train}\")\n",
    "    print(f\"Testing : {len(test_sents)} → {out_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "892a4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split_train_test(OUTPUT_20NG_TOKENIZED_JSONL, \"20N\", GROUP_ID, out_dir=\"splits\")\n",
    "# split_train_test(OUTPUT_BAC_TOKENIZED_JSONL, \"BAC\", GROUP_ID, out_dir=\"splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d997e",
   "metadata": {},
   "source": [
    "## N- Gramas con suavizado de laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdbea0",
   "metadata": {},
   "source": [
    "Se generó una solución que permite procesar archivos JSONL en batches, de manera que no es necesario cargar todo el archivo en memoria. Esta implementación podría ser más eficientre si en un solo recorrido se generaran los n-gramas de 1 a n, pero por simplicidad se decidió hacer un recorrido por cada n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ccaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_probabilities(\n",
    "    sentences: List[List[str]], \n",
    "    n: int = 2, \n",
    "    laplace: float = 1.0,\n",
    "    batch_size: int = 50000\n",
    ") -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de n-gramas usando suavizado de Laplace con procesamiento eficiente en lotes.\n",
    "    \n",
    "    Esta función procesa un corpus de oraciones para calcular las probabilidades de n-gramas\n",
    "    aplicando suavizado de Laplace para manejar n-gramas no vistos. Utiliza procesamiento\n",
    "    por lotes para optimizar el uso de memoria en corpus grandes.\n",
    "    \n",
    "    La probabilidad se calcula como:\n",
    "    - Para unigramas: P(w) = (count(w) + alpha) / (total_words + alpha * vocab_size)\n",
    "    - Para n-gramas: P(wn|w1...wn-1) = (count(w1...wn) + alpha) / (count(w1...wn-1) + alpha * vocab_size)\n",
    "\n",
    "    alpha es el parámetro de suavizado de Laplace, por defecto 1.0.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[List[str]]): Lista de oraciones tokenizadas, donde cada oración \n",
    "            es una lista de strings representando palabras o tokens.\n",
    "        n (int, optional): Tamaño del n-grama a calcular. Por defecto 2 (bigramas).\n",
    "            Debe ser >= 1.\n",
    "        laplace (float, optional): Factor de suavizado de Laplace (alpha). Por defecto 1.0.\n",
    "            Valores más altos proporcionan más suavizado.\n",
    "        batch_size (int, optional): Número de oraciones a procesar por lote para optimizar\n",
    "            memoria. Por defecto 50000.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[Tuple[str, ...], float]: Diccionario donde las claves son tuplas representando\n",
    "            n-gramas y los valores son sus probabilidades calculadas con suavizado de Laplace.\n",
    "        Dict[Tuple[str, ...], float]: Diccionario donde las claves son tuplas representando\n",
    "            contextos (n-1)-grama y los valores son sus conteos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fase 1: Conteo eficiente\n",
    "    ngram_counts = Counter()\n",
    "    context_counts = Counter() if n > 1 else None\n",
    "    vocab_size = 0\n",
    "    total_ngrams = 0\n",
    "    \n",
    "    # Procesar en lotes para memoria\n",
    "    for batch_start in range(0, len(sentences), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sentences))\n",
    "        batch_sentences = sentences[batch_start:batch_end]\n",
    "        \n",
    "        # Set temporal para vocabulario del batch\n",
    "        batch_vocab = set()\n",
    "        \n",
    "        for sent in batch_sentences:\n",
    "            batch_vocab.update(sent)\n",
    "            \n",
    "            # Contar n-gramas\n",
    "            for i in range(len(sent) - n + 1):\n",
    "                ngram = tuple(sent[i:i + n])\n",
    "                ngram_counts[ngram] += 1\n",
    "                total_ngrams += 1\n",
    "                \n",
    "                # Solo calcular contextos si n > 1\n",
    "                if context_counts is not None:\n",
    "                    context = tuple(sent[i:i + n - 1])\n",
    "                    context_counts[context] += 1\n",
    "        \n",
    "        # Actualizar tamaño de vocabulario\n",
    "        vocab_size = len(batch_vocab | set().union(*[\n",
    "            set(sent) for sent in sentences[:batch_start]\n",
    "        ])) if batch_start > 0 else len(batch_vocab)\n",
    "\n",
    "        # Liberar memoria del batch\n",
    "        del batch_vocab\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calcular vocabulario final una sola vez\n",
    "    if batch_size < len(sentences):\n",
    "        all_vocab = set()\n",
    "        for sent in sentences:\n",
    "            all_vocab.update(sent)\n",
    "        vocab_size = len(all_vocab)\n",
    "        del all_vocab\n",
    "    \n",
    "    # Fase 2: Calcular probabilidades\n",
    "    probabilities = {}\n",
    "    \n",
    "    if n == 1:\n",
    "        # Para unigramas - calcular total una sola vez\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            prob = (count + laplace) / (total_ngrams + laplace * vocab_size)\n",
    "            probabilities[ngram] = prob\n",
    "    else:\n",
    "        # Para n-gramas superiores\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            context = ngram[:-1]\n",
    "            context_count = context_counts[context]\n",
    "            prob = (count + laplace) / (context_count + laplace * vocab_size)\n",
    "            probabilities[ngram] = prob\n",
    "    \n",
    "    context_counts_dict = dict(context_counts) if context_counts else None\n",
    "\n",
    "    # Limpiar memoria\n",
    "    del ngram_counts\n",
    "    gc.collect()\n",
    "\n",
    "    return probabilities, context_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c60e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_for_ngrams(\n",
    "    jsonl_file_path: str,\n",
    "    n: int = 2,\n",
    "    laplace: float = 1.0,\n",
    "    batch_size: int = 50000\n",
    ") -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Lee un archivo JSONL con sentencias y calcula probabilidades de n-gramas.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_file_path: Ruta al archivo JSONL\n",
    "        n: Tamaño del n-grama (1=unigrama, 2=bigrama, etc.)\n",
    "        laplace: Factor de suavizado de Laplace\n",
    "        batch_size: Tamaño del lote para procesamiento en memoria\n",
    "        max_sentences: Máximo número de sentencias a procesar (None = todas)\n",
    "        remove_special_tokens: Si remover tokens especiales como <s>, </s>, <UNK>\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con n-gramas como claves y probabilidades como valores\n",
    "        Diccionario con contextos (n-1)-grama como claves y sus conteos como valores\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    print(f\"Leyendo archivo JSONL: {jsonl_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                \n",
    "                line = line.strip()\n",
    "                if not line:  # Saltar líneas vacías\n",
    "                    continue\n",
    "                try:\n",
    "                    # Parsear JSON\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # Extraer sentencia\n",
    "                    if \"sentence\" not in data:\n",
    "                        print(f\"Advertencia: línea {line_num} no tiene clave 'sentence'\")\n",
    "                        continue\n",
    "                    \n",
    "                    sentence = data[\"sentence\"]\n",
    "                    \n",
    "                    # Validar que sea una lista\n",
    "                    if not isinstance(sentence, list):\n",
    "                        print(f\"Advertencia: línea {line_num} - 'sentence' no es una lista\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Saltar sentencias vacías\n",
    "                    if not sentence:\n",
    "                        continue\n",
    "                    \n",
    "                    sentences.append(sentence)\n",
    "                    \n",
    "                \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parseando JSON en línea {line_num}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error procesando línea {line_num}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo: {jsonl_file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error leyendo archivo: {e}\")\n",
    "    \n",
    "    if not sentences:\n",
    "        raise ValueError(\"No se encontraron sentencias válidas en el archivo\")\n",
    "    \n",
    "    print(f\"Total de sentencias cargadas: {len(sentences)}\")\n",
    "    print(f\"Calculando {n}-gramas con suavizado de Laplace (α={laplace})...\")\n",
    "    \n",
    "    # Llamar a la función de n-gramas\n",
    "    probabilities, context_counts = calculate_ngram_probabilities(\n",
    "        sentences=sentences,\n",
    "        n=n,\n",
    "        laplace=laplace,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Cálculo completado. Total de {n}-gramas únicos: {len(probabilities)}\")\n",
    "    if context_counts:\n",
    "        print(f\"Total de contextos únicos: {len(context_counts)}\")\n",
    "    \n",
    "    return probabilities, context_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fbcca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/20N_0100_training.jsonl\n",
      "Total de sentencias cargadas: 230368\n",
      "Calculando 1-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 1-gramas únicos: 72174\n"
     ]
    }
   ],
   "source": [
    "unigrams_20n, context_counts_uni = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_20NG_SPLITS_JSONL,\n",
    "        n=1,\n",
    "        laplace=1.0,\n",
    "        batch_size=50000\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4eb475ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/20N_0100_training.jsonl\n",
      "Total de sentencias cargadas: 230368\n",
      "Calculando 2-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 2-gramas únicos: 941694\n",
      "Total de contextos únicos: 72173\n"
     ]
    }
   ],
   "source": [
    "bigrams_20n, context_counts_bi = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_20NG_SPLITS_JSONL,\n",
    "        n=2,\n",
    "        laplace=1.0,\n",
    "        batch_size=50000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62337005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/20N_0100_training.jsonl\n",
      "Total de sentencias cargadas: 230368\n",
      "Calculando 3-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 3-gramas únicos: 2342730\n",
      "Total de contextos únicos: 939236\n"
     ]
    }
   ],
   "source": [
    "trigrams_20n, context_counts_tri = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_20NG_SPLITS_JSONL,\n",
    "        n=3,\n",
    "        laplace=1.0,\n",
    "        batch_size=50000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "358f97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngrams/unigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(unigrams_20n, f)\n",
    "\n",
    "with open('ngrams/bigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(bigrams_20n, f)\n",
    "\n",
    "with open('ngrams/trigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(trigrams_20n, f)\n",
    "\n",
    "\n",
    "with open('ngrams/context_counts_unigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_uni, f)\n",
    "\n",
    "with open('ngrams/context_counts_bigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_bi, f)\n",
    "\n",
    "with open('ngrams/context_counts_trigrams_20n.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_tri, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af0bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/BAC_0100_training.jsonl\n",
      "Total de sentencias cargadas: 7130342\n",
      "Calculando 1-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 1-gramas únicos: 386329\n"
     ]
    }
   ],
   "source": [
    "unigrams_bac, context_counts_uni_bac = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_BAC_SPLITS_JSONL,\n",
    "        n=1,\n",
    "        laplace=1.0,\n",
    "        batch_size=250000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5e8b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngrams/unigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(unigrams_bac, f)\n",
    "\n",
    "with open('ngrams/context_counts_unigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_uni_bac, f)  # Será None para unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98119c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/BAC_0100_training.jsonl\n",
      "Total de sentencias cargadas: 7130342\n",
      "Calculando 2-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 2-gramas únicos: 9861802\n",
      "Total de contextos únicos: 386328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigrams_bac, context_counts_bi_bac = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_BAC_SPLITS_JSONL,\n",
    "        n=2,\n",
    "        laplace=1.0,\n",
    "        batch_size=250000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61e29575",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngrams/bigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(bigrams_bac, f)\n",
    "\n",
    "with open('ngrams/context_counts_bigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_bi_bac, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78618b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo JSONL: splits/BAC_0100_training.jsonl\n",
      "Total de sentencias cargadas: 7130342\n",
      "Calculando 3-gramas con suavizado de Laplace (α=1.0)...\n",
      "Cálculo completado. Total de 3-gramas únicos: 38191007\n",
      "Total de contextos únicos: 9842678\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trigrams_bac, context_counts_tri_bac = process_jsonl_for_ngrams(\n",
    "        jsonl_file_path=OUTPUT_BAC_SPLITS_JSONL,\n",
    "        n=3,\n",
    "        laplace=1.0,\n",
    "        batch_size=250000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c55a7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngrams/trigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(trigrams_bac, f)\n",
    "\n",
    "with open('ngrams/context_counts_trigrams_bac.pkl', 'wb') as f:\n",
    "    pickle.dump(context_counts_tri_bac, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
